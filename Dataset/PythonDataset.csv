prefix,suffix,middle,filename
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier","import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()","# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint","# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","import warnings

warnings.filterwarnings('ignore')",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []","# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","precisions = []
recalls = []",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],","}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []","X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","# Iterate over different train sizes
for train_size in train_sizes:",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning","param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column","grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()","Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","kfold = KFold(n_splits=10)

'''",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score","import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)","'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","# Define the parameter grid based on the parameter options you provided
param_grid = {",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())","# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","# Remove rows with missing values
cleaned_data = df.dropna()",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)","# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''","# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values","outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)","# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes","Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame","So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")","The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%
accuracies = []
precisions = []
recalls = []

# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","""""""
So, for example, if you look at the row for ""Glucose"":",ClassificationDiabetesDataset.py
"import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
import pprint
import warnings

warnings.filterwarnings('ignore')

# df - data frame
df = pd.read_csv(""E:\Stuff\introai\\09-decision_tree_diabetes\diabetes_changed.csv"")

""""""
So, for example, if you look at the row for ""Glucose"":
The number in the first column (99.00000) means that 25% of the people have glucose levels lower than 99.
The number in the second column (117.0000) is the median, which means half of the people have glucose levels lower than 117 and half have higher.
The number in the third column (140.25000) means that 75% of the people have glucose levels lower than 140.25.
""""""

quantiles = df.iloc[:, :-1].quantile(q=[0.2, 0.5, 0.8], axis=0, numeric_only=True).T
diabetes_distribution = df['Outcome'].value_counts() * 100 / len(df)
print(f""\nPercentage of people with features lower than:\n{quantiles}"")
print(f""\nDistribution of diabetes presence (in %):\n{diabetes_distribution}"")

columns_cant_have_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[columns_cant_have_zeros] = df[columns_cant_have_zeros].replace(0, np.NaN)
print('\n', df[columns_cant_have_zeros])
print('\n Number of NaN values', df.isnull().sum())

# Remove rows with missing values
cleaned_data = df.dropna()

# Group the cleaned data by the 'Outcome' column
outcome_groups = cleaned_data.groupby('Outcome')

# Reset the index within each group to have consecutive row numbers
grouped_data = outcome_groups.apply(lambda group: group.reset_index(drop=True))

# Compute the median for those with and without diabetes
median_diabetes = grouped_data.loc[grouped_data['Outcome'] == 1].median()
median_no_diabetes = grouped_data.loc[grouped_data['Outcome'] == 0].median()

# Print the medians
print(""\nMedian for individuals with diabetes:"")
print(median_diabetes)

print(""\nMedian for individuals without diabetes:"")
print(median_no_diabetes)

for column in columns_cant_have_zeros:
    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_no_diabetes[column]
    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_diabetes[column]

print('\nChecking number of zero values', df.isnull().sum())
# separating output and features
y = df['Outcome']
x = df.iloc[:, :-1]

# dividing data
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.4, random_state=42)

# building model
model = DecisionTreeClassifier()
kfold = KFold(n_splits=10)

'''
Cross-validation is a technique used in machine learning to assess the performance of a predictive model. It helps to evaluate how well the model will generalize to new, unseen data. The basic idea behind cross-validation is to split the dataset into multiple subsets, train the model on some of these subsets, and then evaluate its performance on the remaining subsets.
'''

# estimating accuracy
cv_results = cross_val_score(model, x, y, cv = 10, scoring='accuracy')

# Classification and Regression Trees (CART)
print(f""\nDecision tree using cross-val: \n\tMean accuracy: {round(cv_results.mean(), 5)} \n\tStd: {round(cv_results.std(), 5)}"")

decision_tree = DecisionTreeClassifier()
decision_tree = decision_tree.fit(X_train,Y_train)
y_predicted = decision_tree.predict(X_test)

# Classification and Regression Trees (CART)
print(f""\nDecision tree no cross-val: \n\tAccuracy: {round(accuracy_score(Y_test, y_predicted), 5)}"" f""\n\tPrecision: {round(precision_score(Y_test, y_predicted), 5)}"" f""\n\tRecall: {round(recall_score(Y_test, y_predicted), 5)}"")

# model tuning
decision_tree = DecisionTreeClassifier(random_state=42)

# Define the parameter grid based on the parameter options you provided
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [5, 6, 7, 8, 9, 10, ],
    'min_samples_split': [1, 2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8, 16],
    'max_features': ['auto', 'sqrt', 'log2', None],
    'ccp_alpha': [0, .001, .005, .01, .05, .1]
}

print('\n')
# GridSearchCV with the decision tree and the parameter grid
grid_search = GridSearchCV(estimator=decision_tree, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)

# fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)

# best parameters and the best score
print(""Best parameters found: "", grid_search.best_params_)
print(""Best cross-validation score: {:.5f}"".format(grid_search.best_score_))

# using the best estimator to make predictions
best_tree = grid_search.best_estimator_
y_pred = best_tree.predict(X_test)

print(""Accuracy on test set: {:.5f}"".format(accuracy_score(Y_test, y_pred)))
print(""Precision on test set: {:.5f}"".format(precision_score(Y_test, y_pred)))
print(""Recall on test set: {:.5f}"".format(recall_score(Y_test, y_pred)))

best_params = grid_search.best_params_
model = DecisionTreeClassifier(**best_params)

train_sizes = np.linspace(0.1, 0.9, 9)  # Train sizes from 10% to 90%","# Iterate over different train sizes
for train_size in train_sizes:
    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size, random_state=42)
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(Y_test, Y_pred))
    precisions.append(precision_score(Y_test, Y_pred))
    recalls.append(recall_score(Y_test, Y_pred))

# Plotting Accuracy, Precision, and Recall vs. Train-Test Proportion
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, accuracies, label='Accuracy')
plt.plot(train_sizes, precisions, label='Precision')
plt.plot(train_sizes, recalls, label='Recall')
plt.xlabel('Proportion of Training Data')
plt.ylabel('Performance Metrics')
plt.title('Model Performance vs. Train-Test Split Proportion')
plt.legend()
plt.grid(True)
plt.show()

# Visualizing the Decision Tree using the last trained model
plt.figure(figsize=(20, 10))
plot_tree(model, filled=True, feature_names=df.columns[:-1], class_names=[""No Diabetes"", ""Diabetes""])
plt.title('Decision Tree Visualization')
plt.show()","accuracies = []
precisions = []
recalls = []",ClassificationDiabetesDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome","val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')","plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input","Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","# task 2 and 3
    '''",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)","return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","# For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''","0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".","def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models","import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","from keras import layers
from keras.utils import to_categorical
import numpy as np",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]","# for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","#      
    models_l = []",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46","(train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure","plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()","acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)",plt.clf()   # clear figure,ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""","One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)","one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)","print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]","The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","# task 4
    '''",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]","train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","train_data = train[1000:]

    val_labels = train_labels[:1000]",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set","'''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","7.     
    8.    87%,  82.4%",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),","model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''","3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    #      
    models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    """"""
    Data = a newswire represented as a sequence of integers (representing words) 
    Labels = one of 46 categories the newswire talks about
    Consider only the 10000 most often used words, vectorization produces a vector of length 10000, index i = 1 if word i is in the newswire
    One hot labels is a vector of length 46 with 1 at the position of the correct category
    """"""
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    # For fun, we can decode the input data to see what a newswire looks like
    # decode_input_data(train_data)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    """"""
    Create a network with the input of size 10000, two hidden layers, and one output layer of size 46
    The output of the network is a vector of probabilities the newswire falls into the specific category
    Set aside 1000 samples for validation, use the rest for training
    """"""
    (train,train_labels,_,_) = input

    # task 2 and 3
    '''
    Using significantly fewer neurons than the output size in a neural network may lead to a bottleneck effect, limiting the network's capacity to capture complex patterns and reducing its ability to learn from the data effectively, potentially resulting in underfitting and poor performance. It can also increase the risk of information loss and hinder the model's ability to represent the input data's full complexity, impacting its predictive power.
    '''
    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    # model.add(layers.Dropout(0.2))
    # model.add(layers.Dense(256, activation='silu'))
    model.add(layers.Dense(46, activation='softmax'))
    '''    
    Milestones (    Grid Search)
    1.  -   64  128,   2  1.    .     batch_size
    2.  Learning Rate Schedule (CosineDecay)     len(train) * epoch_num,  initial Learning Rate
    3.   0.2
    4.    hidden layers  silu
    5.  label_smoothing=0.1
    6.  allback          val set
    7.     
    8.    87%,  82.4%

    '''   

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    # task 4
    '''
    The disparity in accuracy and loss between the training and validation sets typically suggests that the model is overfitting. When accuracy on the training set is high while accuracy on the validation set lags behind, it indicates that the model is memorizing the training data rather than generalizing well to unseen data. This observation implies that the model may need to be trained for fewer epochs or that regularization techniques should be employed to prevent overfitting and improve generalization performance.
    '''
    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    """"""
    History contains data about the training process. It contains an entry for each metric used for both training and validation.
    Specifically, we plot loss = difference between the expected outcome and the produced outcome
    and accuracy = fraction of predictions the model got right
    """"""
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":

    # prepare data (task 1)
    '''
    The model's performance on the testing set is more comparable to its performance on the validation set, as both serve to estimate its generalization ability to unseen data, while the training set is used for parameter optimization.
    '''
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]","models_l = []
    # for i in range(3):
    #     # create and train the neural network
    #     (history,model) = create_and_train_network(input, i)
    #      # show the results
    #     print_graphs(history)

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","val_labels = train_labels[:1000]

    #      ",ClassificationReutersDataset.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")","mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","else:
            mip_image_normalized = mip_image",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.","""""""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()",":param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""","img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:","# Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image","print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):","img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","""""""
    Normalize image to 0-255 and convert to uint8.
    """"""",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)","mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)","def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')","if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","# Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(","num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","process_channel,
                        series_idx,
                        channel,",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties","# Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:","if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask","pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","_, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.","if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()",":param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images","if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","for z in range(num_images):
        img = image_arrays[z]",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:","if __name__ == ""__main__"":
    main()","print(f""Error processing a channel: {e}"")

    print(""Processing complete."")",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)","""""""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map","pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)","# Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","# Set background to black
    pseudocolor_img[mask == 0] = 0",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return","print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","# Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")",FileExtractionPseudocolorMIP.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c","d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def decrypt(privateKey, message):
    # Unpack the key",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):","while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","return True

    d = n - 1",RivestShamirAdleman.py
"import random
import math","if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))","print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted","# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2","N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","print(""P is "" + str(P))
print(""Q is "" + str(Q))",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1","check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):","# Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","return ((e, N), (d, N))

def encrypt(publicKey, message):",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))","while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","phi = (P-1)*(Q-1)
print("" is "" + str(phi))",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:","Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","P = P | 1
    P = P + 2",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2","print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds","P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False","# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)",return True,RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)","if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption","print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","M = decrypt(privateKey, C)

print('Cipher text is:  ', C)",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)","e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","print("" is "" + str(phi))

while True:",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2","if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","if (x == 1):
            return False",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))","print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","if g == 1 and check == 1:
        break",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N","def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","c = [pow(ord(char), e, n) for char in message]
    return c",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c","# Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey",RivestShamirAdleman.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop","model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","best_val_loss = float('inf')

for epoch in range(EPOCHS):",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt","NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# hyperparameters
BATCH_SIZE = 4",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0","optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)","target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)","val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","outputs = model(images)
            loss = loss_fn(outputs, masks)",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# metrics
def calculate_iou(pred, target, num_classes):
    ious = []",UNetTrain.py
"import os
import torch
import torch.nn as nn","import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(","if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)","iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# validation
    model.eval()
    val_loss = 0.0",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset","# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)","val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# validation
    model.eval()",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])","# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)","val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8","# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)","# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()","for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","val_loss = 0.0
    iou_scores = []
    with torch.no_grad():",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# model itself
# we can choose a pre-trained encoder, 'resnet34' for example",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls","ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:",UNetTrain.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""","T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# model itself
# we can choose a pre-trained encoder, 'resnet34' for example
model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

# loss and optimizer
# gonna use CrossEntropyLoss for multiclass segmentation
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics
def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious


# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)",UNetTrain.py
