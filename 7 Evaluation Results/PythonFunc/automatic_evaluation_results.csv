middle,gen_tiny_starcoder_py_Levenshtein,gen_tiny_starcoder_py_Cyclomatic_Complexity,gen_tiny_starcoder_py_Cyclomatic_Complexity_Diff,gen_tiny_starcoder_py_Exact_Match,gen_tiny_starcoder_py_BLEU,gen_tiny_starcoder_py_ROUGE,gen_tiny_starcoder_py_chrF,gen_tiny_starcoder_py_Cosine_Similarity,gen_starcoder2_3b_Levenshtein,gen_starcoder2_3b_Cyclomatic_Complexity,gen_starcoder2_3b_Cyclomatic_Complexity_Diff,gen_starcoder2_3b_Exact_Match,gen_starcoder2_3b_BLEU,gen_starcoder2_3b_ROUGE,gen_starcoder2_3b_chrF,gen_starcoder2_3b_Cosine_Similarity,gen_starcoder2_7b_Levenshtein,gen_starcoder2_7b_Cyclomatic_Complexity,gen_starcoder2_7b_Cyclomatic_Complexity_Diff,gen_starcoder2_7b_Exact_Match,gen_starcoder2_7b_BLEU,gen_starcoder2_7b_ROUGE,gen_starcoder2_7b_chrF,gen_starcoder2_7b_Cosine_Similarity,gen_starcoder2_15b_Levenshtein,gen_starcoder2_15b_Cyclomatic_Complexity,gen_starcoder2_15b_Cyclomatic_Complexity_Diff,gen_starcoder2_15b_Exact_Match,gen_starcoder2_15b_BLEU,gen_starcoder2_15b_ROUGE,gen_starcoder2_15b_chrF,gen_starcoder2_15b_Cosine_Similarity
"def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)",0.8140417457305503,,1,0,0.0076120823233858425,0.046153846153846156,0.0,0.41876848959783736,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.611904761904762,1.0,0,0,0.2806220089099083,0.5471698113207548,0.0,0.8328918881308462,0.4666666666666667,1.0,0,0,0.4178067996159888,0.6481481481481481,0.0,0.865270262953244
"def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()",0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0
"def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results",0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.3994082840236686,2.0,0,0,0.6218943633527284,0.6857142857142856,0.0,0.7546694635475508,0.21621621621621623,2.0,0,0,0.8143230312853286,0.8571428571428571,0.0,0.8574488506117108
"def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)",0.978448275862069,0.0,3,0,2.126604667302e-15,0.0,0.0,0.14110458371249512,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.9525862068965517,0.0,3,0,2.686521845175611e-07,0.05714285714285715,0.0,0.23946719433141653,0.8383620689655172,0.0,3,0,0.011933313067617607,0.045454545454545456,0.0,0.13034772818854024
"def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)",0.9837340876944838,0.0,1,0,1.0136961597081952e-42,0.012658227848101266,0.0,0.11516899058665901,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.8330975954738331,1.0,0,0,0.01856050226652858,0.19487179487179487,0.0,0.5634091745977596
"def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)",0.9952898550724638,0.0,9,0,2.808428841844002e-71,0.005509641873278237,0.0,0.13867376994883243,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.85,,9,0,0.003524656399744896,0.18140589569161,0.0,0.693708902621625,0.7960144927536232,3.0,6,0,0.01877833698577388,0.14314115308151093,0.0,0.472370942921195
"def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)",0.8389513108614233,,1,0,9.860779815994042e-05,0.27906976744186046,0.0,0.5812829330605702,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.6656760772659732,3.0,-2,0,0.11465118436635248,0.36231884057971014,0.0,0.7250095105066872
"def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized",0.934375,0.0,2,0,0.0003619215899570381,0.0,0.0,0.04068446221194623,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.5934065934065934,1.0,1,0,0.15730895780977475,0.32608695652173914,0.0,0.7812152835433653
"def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")",0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.7209653092006033,1.0,2,0,0.07837256047478186,0.1931034482758621,0.0,0.6027708451344913
"def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")",0.9769484083424808,,5,0,5.1953895603425654e-12,0.010256410256410256,0.0,0.206290421474846,0.8084522502744237,,5,0,0.02131010678091329,0.20155038759689925,0.0,0.7437469130503493,0.8556531284302964,,5,0,0.007397073225119665,0.12648221343873517,0.0,0.6043870972548351,0.6981339187705817,,5,0,0.11300925842400597,0.18666666666666665,0.0,0.6768722361062685
"def main(args: argparse.Namespace) -> tuple[list[float], float, float]:
    # Create a random generator with a given seed.
    generator = np.random.RandomState(args.seed)

    # Generate an artificial regression dataset.
    data, target = sklearn.datasets.make_regression(n_samples=args.data_size, random_state=args.seed)

    data = np.hstack([data, np.ones(shape=(data.shape[0], 1))])

    # TODO: Split the dataset into a train set and a test set.
    train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(data, target, test_size=args.test_size, random_state=args.seed)

    # Generate initial linear regression weights.
    weights = generator.uniform(size=train_data.shape[1], low=-0.1, high=0.1)

    train_rmses, test_rmses = [], []
    for epoch in range(args.epochs):
        permutation = generator.permutation(train_data.shape[0])

        # TODO: Process the data in the order of `permutation`.
        for i in range(0, train_data.shape[0], args.batch_size):
            batch = train_data[permutation[i:i + args.batch_size]]
            batch_target = train_target[permutation[i:i + args.batch_size]]

            predictions = batch.dot(weights)
            error = predictions - batch_target
            gradient = batch.T.dot(error) / batch.shape[0]

            # L2 regularization
            weights_no_bias = np.copy(weights)
            weights_no_bias[-1] = 0  # here we exclude bias from regularization
            gradient = gradient + args.l2 * weights_no_bias

            weights = weights - args.learning_rate * gradient

        # TODO: Append current RMSE on train/test to `train_rmses`/`test_rmses`.
        train_predictions = train_data.dot(weights)
        train_rmse = np.sqrt(mean_squared_error(train_predictions, train_target))
        train_rmses.append(train_rmse)

        # RMSE on the test set
        test_predictions = test_data.dot(weights)
        test_rmse = np.sqrt(mean_squared_error(test_predictions, test_target))
        test_rmses.append(test_rmse)

    # TODO: Compute into `explicit_rmse` test data RMSE when fitting
    # `sklearn.linear_model.LinearRegression` on `train_data` (ignoring `args.l2`).
    model = LinearRegression()
    model.fit(train_data, train_target)
    explicit_predictions = model.predict(test_data)
    explicit_rmse = np.sqrt(mean_squared_error(test_target, explicit_predictions))

    if args.plot:
        import matplotlib.pyplot as plt
        plt.plot(train_rmses, label=""Train"")
        plt.plot(test_rmses, label=""Test"")
        plt.xlabel(""Epochs"")
        plt.ylabel(""RMSE"")
        plt.legend()
        plt.show() if args.plot is True else plt.savefig(args.plot, transparent=True, bbox_inches=""tight"")

    return weights, test_rmses[-1], explicit_rmse",0.9834390415785764,,5,0,9.79588375177932e-24,0.015706806282722516,0.0,0.2976671557508479,0.8974630021141649,1.0,4,0,0.00022553033484754955,0.09216589861751151,0.0,0.5580017507169848,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.817477096546864,3.0,2,0,0.018367023705418956,0.20833333333333337,0.0,0.6416355065855714
"def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True",0.8020477815699659,2.0,5,0,0.046311358551130735,0.3333333333333333,0.0,0.6421676766969919,0.6928327645051194,5.0,2,0,0.11941513404533029,0.3125,0.0,0.7701723544386725,0.5664893617021277,4.0,3,0,0.12573427657326006,0.26785714285714285,0.0,0.7491436073507548,0.07508532423208192,7.0,0,0,0.9264593770522732,0.9655172413793104,0.0,0.9756892190065365
"def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c",0.7625,0.0,2,0,0.10258703827753633,0.22857142857142856,0.0,0.5927346934059503,0.652267818574514,,2,0,0.17880248429424617,0.3893805309734513,0.0,0.6926872845738608,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.3568904593639576,2.0,0,0,0.4423218678146712,0.691358024691358,0.0,0.9455592319926461
"def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False",0.9518072289156626,0.0,6,0,0.0,0.03508771929824561,0.0,0.29029520150841615,0.6206896551724138,,6,0,0.1692247818767118,0.43478260869565216,0.0,0.8890026651872451,0.375366568914956,7.0,-1,0,0.4215503093673728,0.6915887850467289,0.0,0.9146661107579215,0.0,0.0,0,0,0.0,0.0,0.0,0.0
"def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)",0.9372197309417041,,2,0,1.6079326079993796e-06,0.10810810810810811,0.0,0.4788924299318616,0.3901345291479821,2.0,0,0,0.5145868449926518,0.6268656716417911,0.0,0.8166455320748849,0.3284671532846715,2.0,0,0,0.4858678015544508,0.6075949367088608,0.0,0.8075823222155067,0.43223443223443225,2.0,0,0,0.3895722441949229,0.5974025974025974,0.0,0.7757645935869812
"def defineKeypair (e, d, N):
    return ((e, N), (d, N))",0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.5789473684210527,1.0,0,0,0.5375396637628254,0.75,0.0,0.9255082217414079,0.8156996587030717,1.0,0,0,0.24768639582188734,0.4166666666666667,0.0,0.621198102289936
"def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious",0.686046511627907,1.0,2,0,0.07144033370767561,0.22580645161290325,0.0,0.7831783093172927,0.7093023255813954,1.0,2,0,0.05282598075114636,0.3018867924528302,0.0,0.683263368224158,0.6877076411960132,,3,0,0.1721394104402385,0.35820895522388063,0.0,0.6411756791408035,0.3654485049833887,3.0,0,0,0.58081363164596,0.761904761904762,0.0,0.933520188546966
