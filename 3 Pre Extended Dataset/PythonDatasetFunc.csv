prefix,suffix,middle,filename
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt","def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    models_l = []

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)","if __name__ == ""__main__"":
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    models_l = []

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)","def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)

def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    models_l = []

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results","def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)



def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    models_l = []

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)",ClassificationReutersDataset.py
"import keras
from keras.datasets import reuters
from keras import models
from keras import layers
from keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

def create_testing_data():
    (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

    train = vectorize_sequences(train_data)
    test = vectorize_sequences(test_data)
    one_hot_train_labels = to_categorical(train_labels)
    one_hot_test_labels = to_categorical(test_labels)

    return (train, one_hot_train_labels, test, one_hot_test_labels)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

def decode_input_data(train_data):
    word_index = reuters.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    # Note that our indices were offset by 3
    # because 0, 1 and 2 are reserved indices for ""padding"", ""start of sequence"", and ""unknown"".
    decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])
    print(decoded_newswire)","def print_graphs(history):
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs = range(1, len(loss) + 1)

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

    plt.clf()   # clear figure

    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()


if __name__ == ""__main__"":
    input = create_testing_data()
    (train,train_labels,_,_) = input
    val_data = train[:1000]
    val_labels = train_labels[:1000]

    models_l = []

    for i in range(3):
        model = keras.models.load_model(f'checkpoint_mode_{i + 1}.keras')
        model.fit(train, train_labels)
        models_l.append(model)

    ensemble_input = keras.layers.Input(shape=(10000,))
    outputs = [model(ensemble_input) for model in models_l]
    ensemble_output = keras.layers.Average()(outputs)

    ensemble = keras.Model(inputs=ensemble_input, outputs=ensemble_output)
    ensemble.compile(loss=keras.losses.CategoricalCrossentropy(),
                     metrics=['accuracy'])
    
    print(""Dev accuracy: "")
    acc = ensemble.evaluate(val_data, val_labels)","def create_and_train_network(input, index):
    (train,train_labels,_,_) = input

    # specify the shape of the network
    model = models.Sequential()
    model.add(layers.Dropout(0.2))
    model.add(layers.Dense(128, activation='silu', input_shape=(10000,)))
    model.add(layers.Dense(46, activation='softmax'))

    # split input data into training set and validation set
    val_data = train[:1000]
    train_data = train[1000:]

    val_labels = train_labels[:1000]
    train_labels = train_labels[1000:]

    cos_dec = keras.optimizers.schedules.CosineDecay(
        0.0001,
        len(train) * 25,
    )

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cos_dec), 
                loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
                metrics=['accuracy'])
    
    checkpoint_filepath = f'checkpoint_mode_{index + 1}.keras'
    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        monitor='val_accuracy',
        mode='max',
        save_best_only=True)

    # train the network
    history = model.fit(train_data,
                        train_labels,
                        epochs=30,
                        batch_size=32,
                        validation_data=(val_data, val_labels),
                        callbacks=[model_checkpoint_callback])
    
    return (history,model)",ClassificationReutersDataset.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized","def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True","def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")","def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)","def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)

def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")

if __name__ == ""__main__"":
    main()","def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")",FileExtractionPseudocolorMIP.py
"import os
import numpy as np
import tifffile
import cv2
from concurrent.futures import ProcessPoolExecutor, as_completed

# Constants
TIFF_FILE_PATH = r""D:\Cilia test\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI\RPE1_FHmNG_MmKatnal2i1_aCEP164_A555_aARL13B_CY5_DAPI.ome.tiff""
OUTPUT_BASE_DIR = os.path.dirname(TIFF_FILE_PATH)
MAX_INTENSITY_PROJECTION_ENABLED = True
PSEUDOCOLOR_ENABLED = True

def max_intensity_projection(images):
    """"""
    Create a maximum intensity projection from a stack of images.
    :param images: 3D numpy array (num_images, height, width).
    :return: 2D numpy array representing the MIP.
    """"""
    return np.max(images, axis=0)

def apply_pseudocolor(image_path):
    """"""
    Apply pseudocolor to a grayscale image, keeping the background black.
    :param image_path: Path to the grayscale image.
    """"""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f""Error: Unable to load image at {image_path}."")
        return

    # Check the data type
    print(f""Pseudocolor - Image dtype before normalization: {img.dtype}"")
    print(f""Pseudocolor - Image min: {img.min()}, max: {img.max()}"")

    # If image is not 8-bit, normalize it
    if img.dtype != np.uint8:
        img = normalize_image(img)
        print(f""Pseudocolor - Image dtype after normalization: {img.dtype}"")
        print(f""Pseudocolor - Image min after normalization: {img.min()}, max after normalization: {img.max()}"")

    # Apply threshold to create mask
    _, mask = cv2.threshold(img, 2, 255, cv2.THRESH_BINARY)

    # Apply pseudocolor map
    pseudocolor_img = cv2.applyColorMap(img, cv2.COLORMAP_VIRIDIS)

    # Set background to black
    pseudocolor_img[mask == 0] = 0

    # Save pseudocolored image
    directory, filename = os.path.split(image_path)
    output_path = os.path.join(directory, f'pseudocolor_{filename}')
    cv2.imwrite(output_path, pseudocolor_img)
    print(f""Pseudocolor image saved at: {output_path}"")

def normalize_image(img):
    """"""
    Normalize image to 0-255 and convert to uint8.
    """"""
    img_min = img.min()
    img_max = img.max()
    if img_max == img_min:
        return np.zeros_like(img, dtype=np.uint8)
    normalized = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)
    return normalized

def process_channel(series_idx, channel, num_images, image_arrays, output_dir):
    """"""
    Process a single channel: save individual images, create MIP, and apply pseudocolor.
    :param series_idx: Index of the series.
    :param channel: Channel number.
    :param num_images: Number of Z-slices.
    :param image_arrays: 3D numpy array (num_images, height, width) for the channel.
    :param output_dir: Directory to save the output images.
    """"""
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Check image array properties
    print(f""Processing Series {series_idx + 1}, Channel {channel + 1}"")
    print(f""Image array dtype: {image_arrays.dtype}, min: {image_arrays.min()}, max: {image_arrays.max()}"")

    # Determine if normalization is needed
    need_normalization = False
    if image_arrays.dtype != np.uint8:
        need_normalization = True
        image_min = image_arrays.min()
        image_max = image_arrays.max()
        print(f""Channel {channel + 1} - Original dtype: {image_arrays.dtype}, min: {image_min}, max: {image_max}"")

    # Save all images
    for z in range(num_images):
        img = image_arrays[z]

        if need_normalization:
            img_to_save = normalize_image(img)
        else:
            img_to_save = img

        image_path = os.path.join(output_dir, f'image_{z + 1}.png')

        # Save image using cv2.imwrite
        success = cv2.imwrite(image_path, img_to_save)
        if not success:
            print(f""Failed to save image at {image_path}"")

    print(f""All images for channel {channel + 1} in series {series_idx + 1} saved to {output_dir}."")

    if MAX_INTENSITY_PROJECTION_ENABLED:
        mip_image = max_intensity_projection(image_arrays)
        print(f""MIP - dtype before normalization: {mip_image.dtype}, min: {mip_image.min()}, max: {mip_image.max()}"")

        # Determine if normalization is needed
        if mip_image.dtype != np.uint8:
            mip_image_normalized = normalize_image(mip_image)
            print(f""MIP - dtype after normalization: {mip_image_normalized.dtype}, min: {mip_image_normalized.min()}, max: {mip_image_normalized.max()}"")
        else:
            mip_image_normalized = mip_image

        mip_image_path = os.path.join(output_dir, 'max_intensity_projection.png')
        success = cv2.imwrite(mip_image_path, mip_image_normalized)
        if not success:
            print(f""Failed to save MIP image at {mip_image_path}"")
        else:
            print(f""Max intensity projection saved for channel {channel + 1} in series {series_idx + 1} at {mip_image_path}."")

            if PSEUDOCOLOR_ENABLED:
                apply_pseudocolor(mip_image_path)","if __name__ == ""__main__"":
    main()","def main():
    with tifffile.TiffFile(TIFF_FILE_PATH) as tif:
        series_count = len(tif.series)
        print(f""Total series to process: {series_count}"")

        # Prepare all tasks for parallel processing
        tasks = []
        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
            for series_idx, series in enumerate(tif.series):
                print(f""Processing series {series_idx + 1} of {series_count}"")

                # Read all image data for the current series at once
                image_data = series.asarray()  # Shape: (Z, C, Y, X)
                print(f""Series {series_idx + 1} shape: {image_data.shape}, dtype: {image_data.dtype}"")

                num_z, num_channels, height, width = image_data.shape  # Assuming shape order is (Z, C, Y, X)

                for channel in range(num_channels):
                    channel_dir = os.path.join(OUTPUT_BASE_DIR, f'series_{series_idx + 1}', f'channel_{channel + 1}')

                    # Extract all images for this channel
                    channel_images = image_data[:, channel, :, :]  # Shape: (Z, Y, X)

                    # Submit processing task
                    tasks.append(executor.submit(
                        process_channel,
                        series_idx,
                        channel,
                        num_z,
                        channel_images,
                        channel_dir
                    ))

            # Optionally, monitor task completion
            for future in as_completed(tasks):
                try:
                    future.result()
                except Exception as e:
                    print(f""Error processing a channel: {e}"")

    print(""Processing complete."")",FileExtractionPseudocolorMIP.py
"#!/usr/bin/env python3
import argparse

import numpy as np
import sklearn.datasets
import sklearn.linear_model
from sklearn.metrics import mean_squared_error
import sklearn.model_selection
from sklearn.linear_model import LinearRegression

parser = argparse.ArgumentParser()
# These arguments will be set appropriately by ReCodEx, even if you change them.
parser.add_argument(""--batch_size"", default=10, type=int, help=""Batch size"")
parser.add_argument(""--data_size"", default=100, type=int, help=""Data size"")
parser.add_argument(""--epochs"", default=50, type=int, help=""Number of SGD training epochs"")
parser.add_argument(""--l2"", default=0.0, type=float, help=""L2 regularization strength"")
parser.add_argument(""--learning_rate"", default=0.01, type=float, help=""Learning rate"")
parser.add_argument(""--plot"", default=False, const=True, nargs=""?"", type=str, help=""Plot the predictions"")
parser.add_argument(""--recodex"", default=False, action=""store_true"", help=""Running in ReCodEx"")
parser.add_argument(""--seed"", default=92, type=int, help=""Random seed"")
parser.add_argument(""--test_size"", default=0.5, type=lambda x: int(x) if x.isdigit() else float(x), help=""Test size"")
# If you add more arguments, ReCodEx will keep them with your default values.","if __name__ == ""__main__"":
    main_args = parser.parse_args([] if ""__file__"" not in globals() else None)
    weights, sgd_rmse, explicit_rmse = main(main_args)
    print(""Test RMSE: SGD {:.3f}, explicit {:.1f}"".format(sgd_rmse, explicit_rmse))
    print(""Learned weights:"", *(""{:.3f}"".format(weight) for weight in weights[:12]), ""..."")","def main(args: argparse.Namespace) -> tuple[list[float], float, float]:
    # Create a random generator with a given seed.
    generator = np.random.RandomState(args.seed)

    # Generate an artificial regression dataset.
    data, target = sklearn.datasets.make_regression(n_samples=args.data_size, random_state=args.seed)

    data = np.hstack([data, np.ones(shape=(data.shape[0], 1))])

    # TODO: Split the dataset into a train set and a test set.
    train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(data, target, test_size=args.test_size, random_state=args.seed)

    # Generate initial linear regression weights.
    weights = generator.uniform(size=train_data.shape[1], low=-0.1, high=0.1)

    train_rmses, test_rmses = [], []
    for epoch in range(args.epochs):
        permutation = generator.permutation(train_data.shape[0])

        # TODO: Process the data in the order of `permutation`.
        for i in range(0, train_data.shape[0], args.batch_size):
            batch = train_data[permutation[i:i + args.batch_size]]
            batch_target = train_target[permutation[i:i + args.batch_size]]

            predictions = batch.dot(weights)
            error = predictions - batch_target
            gradient = batch.T.dot(error) / batch.shape[0]

            # L2 regularization
            weights_no_bias = np.copy(weights)
            weights_no_bias[-1] = 0  # here we exclude bias from regularization
            gradient = gradient + args.l2 * weights_no_bias

            weights = weights - args.learning_rate * gradient

        # TODO: Append current RMSE on train/test to `train_rmses`/`test_rmses`.
        train_predictions = train_data.dot(weights)
        train_rmse = np.sqrt(mean_squared_error(train_predictions, train_target))
        train_rmses.append(train_rmse)

        # RMSE on the test set
        test_predictions = test_data.dot(weights)
        test_rmse = np.sqrt(mean_squared_error(test_predictions, test_target))
        test_rmses.append(test_rmse)

    # TODO: Compute into `explicit_rmse` test data RMSE when fitting
    # `sklearn.linear_model.LinearRegression` on `train_data` (ignoring `args.l2`).
    model = LinearRegression()
    model.fit(train_data, train_target)
    explicit_predictions = model.predict(test_data)
    explicit_rmse = np.sqrt(mean_squared_error(test_target, explicit_predictions))

    if args.plot:
        import matplotlib.pyplot as plt
        plt.plot(train_rmses, label=""Train"")
        plt.plot(test_rmses, label=""Test"")
        plt.xlabel(""Epochs"")
        plt.ylabel(""RMSE"")
        plt.legend()
        plt.show() if args.plot is True else plt.savefig(args.plot, transparent=True, bbox_inches=""tight"")

    return weights, test_rmses[-1], explicit_rmse",LinearRegression.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False","# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))","def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c",RivestShamirAdleman.py
"import random
import math","def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))

def defineKeypair (e, d, N):
    return ((e, N), (d, N))

def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c","publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)",RivestShamirAdleman.py
"import random
import math

def Rabin_Miller_Test(d, n):
    a = random.randrange(2, n-2)
    x = pow(a, d, n) # pow(base, exp, mod)
    if (x == 1 or x == n-1):
        return True
    while (d != n - 1):
        x = (x * x) % n
        d *= 2
 
        if (x == 1):
            return False
        if (x == n - 1):
            return True

    return False

def isPrime (n, k):
    if (n <= 1 or n == 4): # quick dummy check
        return False
    if (n <= 3):
        return True

    d = n - 1
    while (d % 2 == 0):
        d //= 2
 
    for i in range(k):
        if (Rabin_Miller_Test(d, n) == False):
            return False
 
    return True

# rounds = math.log(n)/math.log(4) - number of needed rounds
P = random.randrange(300000000000000000, 10000000000000000000)
P = P | 1
while isPrime (P, 8) == False:
    P = P | 1
    P = P + 2

Q = random.randrange(300000000000000000, 100000000000000000000)
Q = Q | 1
while isPrime (Q, 8) == False:
    Q = Q | 1
    Q = Q + 2

print(""P is "" + str(P))
print(""Q is "" + str(Q))

N = (P * Q)
print(""Modulo is "" + str(N))

phi = (P-1)*(Q-1)
print("" is "" + str(phi))

while True:
    e = random.randrange(2, phi-1)
    e = e | 1
    while isPrime (e, 8) == False:
        e = e | 1
        e = e + 2
    g = math.gcd(e, phi)
    d = pow(e, -1, phi)
    check = d*e % phi
    print(""Check is "" + str(check))
    print(""GCD is "" + str(g))
    if g == 1 and check == 1:
        break

print(""e is "" + str(e))
print(""d is "" + str(d))","def encrypt(publicKey, message):
    # Unpack the key 
    e, n = publicKey
    # Convert each letter of plaintext to ascii using plain^e mod N
    c = [pow(ord(char), e, n) for char in message]
    return c

def decrypt(privateKey, message):
    # Unpack the key 
    d, n = privateKey
    # Generate the plaintext using cipher^d mod N
    p = [chr(pow(char, d, n)) for char in message]
    # Return the array 
    return ''.join(p)

publicKey , privateKey = defineKeypair (e, d, N)

# Entering the message to be encrypted
M = input('Enter your message:  ')
print('Public Key [e,n] = ', publicKey)

# Encryption
C = encrypt(publicKey, M)
# Decryption
M = decrypt(privateKey, C)

print('Cipher text is:  ', C)
print('Private Key [d,n] = ', privateKey)
print('Plain text after decryption is: ', M)","def defineKeypair (e, d, N):
    return ((e, N), (d, N))",RivestShamirAdleman.py
"import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import transforms as T
import segmentation_models_pytorch as smp
from utils import MicroscopyDataset
import numpy as np
import matplotlib.pyplot as plt

# hyperparameters
BATCH_SIZE = 4
NUM_CLASSES = 3  #(e.g., background, cilia, nuclei)
EPOCHS = 50
LEARNING_RATE = 1e-4
TRAIN_SPLIT = 0.8
VALID_SPLIT = 0.2
DEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
MODEL_SAVE_PATH = ""models/unet_model.pth""

# Transforms
transform = T.Compose([
    T.Resize((256, 256)),  # resize images to 256x256 (need to figure out how to enhance size or split to patches)
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalization
])

# dataset
dataset = MicroscopyDataset(images_dir='data/images/', masks_dir='data/masks/', transform=transform)

# split dataset
train_size = int(TRAIN_SPLIT * len(dataset))
valid_size = len(dataset) - train_size
train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])

# data Loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

model = smp.Unet(
    encoder_name=""resnet34"",
    encoder_weights=""imagenet"",  # pre-trained weights
    in_channels=1,  # grayscale
    classes=NUM_CLASSES
)

model = model.to(DEVICE)

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


# metrics","# training Loop
best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    for images, masks in train_loader:
        images = images.to(DEVICE)
        masks = masks.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)

    train_loss = train_loss / len(train_loader.dataset)

    # validation
    model.eval()
    val_loss = 0.0
    iou_scores = []
    with torch.no_grad():
        for images, masks in valid_loader:
            images = images.to(DEVICE)
            masks = masks.to(DEVICE)

            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs, dim=1)
            ious = calculate_iou(preds, masks, NUM_CLASSES)
            iou_scores.append(ious)

    val_loss = val_loss / len(valid_loader.dataset)
    iou_scores = np.nanmean(iou_scores, axis=0)
    mean_iou = np.nanmean(iou_scores)

    print(
        f""Epoch [{epoch + 1}/{EPOCHS}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Mean IoU: {mean_iou:.4f}"")

    # saving the best obtained model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(""Model saved!"")

print(""Training Completed."")","def calculate_iou(pred, target, num_classes):
    ious = []
    pred = pred.view(-1)
    target = target.view(-1)
    for cls in range(num_classes):
        pred_inds = pred == cls
        target_inds = target == cls
        intersection = (pred_inds[target_inds]).long().sum().item()
        union = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection
        if union == 0:
            ious.append(float('nan'))  # If no ground truth, do not include in evaluation
        else:
            ious.append(float(intersection) / float(max(union, 1)))
    return ious",UNetTrain.py
