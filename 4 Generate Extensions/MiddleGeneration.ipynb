{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip uninstall -q bitsandbytes\n",
    "!pip install -U -q bitsandbytes"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import re\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import torch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "drive.mount('/content/drive')"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'm going to define some basic parameters that will be same for all models. Convenient and in one place\n",
    "\n",
    "Also, I'm downloading models with quantization as it allows to better fit into memory and doesnt affect performance much"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = 'cuda'\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parameteres = {\n",
    "    'max_new_tokens': 130,\n",
    "    'repetition_penalty': 1.20,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.1,\n",
    "    'temperature': 0.2,\n",
    "    'do_sample': True\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tiny Starcoder is fast and literally \"tiny\" enough to run as it is"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You might want to commend the biggest model as it doesnt fit into the memory with other models together\n",
    "models = {\n",
    "          'tiny_starcoder_py': {'quantization': False},\n",
    "          'starcoder2-3b': {'quantization': True},\n",
    "          'starcoder2-7b': {'quantization': True},\n",
    "          'starcoder2-15b': {'quantization': True},\n",
    "          }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set your path for dataset with extracted code\n",
    "dataset_path = \"/content/drive/MyDrive/JB2024/PythonDatasetLine.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head(3) # just to be sure"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set your path for dataset with extended extracted code + completions\n",
    "save_path = \"/content/drive/MyDrive/JB2024/PythonDatasetLineExtended.csv\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here I decided to write codes to format code from dataset into needed formating for the model. For starcode based models (and some of others) we use <fim_*> keywords to define different parts of code. So here we format prompt and extract needed parts from generated one"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_prompt(prefix, suffix):\n",
    "    return f\"\"\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\"\"\n",
    "\n",
    "def extract_fim_parts(text):\n",
    "    try:\n",
    "        # Extracting the prefix between <fim_prefix> and <fim_suffix>\n",
    "        prefix_match = re.search(r'<fim_prefix>(.*?)<fim_suffix>', text, re.DOTALL)\n",
    "        prefix_text = prefix_match.group(1).strip() if prefix_match else \"\"\n",
    "\n",
    "        # Extracting the suffix between <fim_suffix> and <fim_middle>\n",
    "        suffix_match = re.search(r'<fim_suffix>(.*?)<fim_middle>', text, re.DOTALL)\n",
    "        suffix_text = suffix_match.group(1).strip() if suffix_match else \"\"\n",
    "\n",
    "        # Extracting the middle part between <fim_middle> and <file_sep> or end of text\n",
    "        middle_match = re.search(r'<fim_middle>(.*?)(<file_sep>|<\\|endoftext\\|>|$)', text, re.DOTALL)\n",
    "        middle_text = middle_match.group(1).strip() if middle_match else \"\"\n",
    "\n",
    "        # some debugging information\n",
    "        print(\"EXTRACTED PARTS:\")\n",
    "        print(\"Prefix:\", prefix_text[:100], \"...\" if len(prefix_text) > 100 else \"\")\n",
    "        print(\"Middle:\", middle_text[:100], \"...\" if len(middle_text) > 100 else \"\")\n",
    "        print(\"Suffix:\", suffix_text[:100], \"...\" if len(suffix_text) > 100 else \"\")\n",
    "\n",
    "        return prefix_text, middle_text, suffix_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting parts: {e}\")\n",
    "        return \"\", \"\", \"\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Two main functions:\n",
    "\n",
    "\n",
    "1.   First initializes model and tokenizer and returns them\n",
    "2.   Second generates code for each middle part with given paremeters and saves to csv (this is esential as I had some moments where due to the error my generated code wasnt saved). You can switch on and off the verbose by the way in this one\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def initialize_model_and_tokenizer(model_name, config, quantization_config=None, device='cpu'):\n",
    "    checkpoint = f'bigcode/{model_name}'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    if config.get('quantization', False):\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_code_snippet(model, tokenizer, dataset, output_column, device='cpu', verbose=True):\n",
    "    for index, row in dataset.iterrows():\n",
    "        prompt = format_prompt(row['prefix'], row['suffix'])\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=torch.ones_like(inputs),\n",
    "            **parameteres\n",
    "        )\n",
    "\n",
    "        prefix, middle, suffix = extract_fim_parts(tokenizer.decode(outputs[0]))\n",
    "        print(f\"MIDDLE PART: {middle}\")\n",
    "\n",
    "        if verbose:\n",
    "            index_str = f\"\\033[90m{str(index + 1)}\\033[00m\"\n",
    "            middle_colored = f\"\\033[90m{middle}\\033[00m\"\n",
    "            filename_colored = f\"\\033[90m{row['filename']}\\033[00m\"\n",
    "            print(f\"Index: {index_str}\\n\"\n",
    "                  f\"Code:\\n{prefix[-250:].lstrip()}{middle_colored}{suffix[:250].rstrip()}\\n\"\n",
    "                  f\"Filename: {filename_colored}\\n\")\n",
    "\n",
    "        dataset.at[index, output_column] = middle\n",
    "        dataset.to_csv(save_path, index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, generating completions for each model and for each middle part. The main loop"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for model_name, config in models.items():\n",
    "    gen_column = 'gen_' + model_name.replace('-', '_')\n",
    "    df[gen_column] = ''  # init new column for each model\n",
    "    # print(quantization_config)\n",
    "\n",
    "    # init model and tokenizer\n",
    "    model, tokenizer = initialize_model_and_tokenizer(model_name, config, quantization_config, device='cuda')\n",
    "\n",
    "    # generate code and update the column for each model\n",
    "    generate_code_snippet(model, tokenizer, df, output_column=gen_column, device='cuda')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some service code for terminating runtime in order not to waste resources"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To sum up, we defined params, defined models we want to use, wrote functions to prepara prompt and work with data, initialized models (and tokenizers) and finally generated completion. After that we will analyze them"
  }
 ]
}
